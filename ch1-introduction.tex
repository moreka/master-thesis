\chapter{Introduction}
Bandit, expert learning, and convex optimisation algorithms have revolutionised online learning, and low regret algorithms are now known for a multitude of diverse settings. Whether the examples are drawn from a distribution, are chosen by an adversary,  come annotated with additional context, or are selected from arbitrary convex domains, there are efficient algorithms that achieve sublinear, $o(T)$, regret after $T$ timesteps. 

One characteristic of these algorithms is that they rarely stick with playing the same action repeatedly, instead, continually exploring new decisions. While this exploration is necessary to achieve low regret, the constant switching between actions can have adverse effects in practice, both from a systems standpoint, and user interface design.  On the systems side,  a lot of switching can wreak havoc on caches, and incur additional latency in the processing of results.  At the same time, most users prefer a sense of predictability, or {\em consistency} in their interactions with the system, and overly dynamic systems add to the overall cognitive load.  

In this work, we investigate the trade-off between the consistency cost (defined as the number of times the algorithm changes its action), and the regret achieved by the algorithm. We show that a simple modification of classic online gradient descent approaches leads to a smooth trade-off between the two goals. At a high level, we achieve this by probabilistically deciding whether to keep playing the same action, or perform an update step. Importantly, we can do this with \emph{constant} additional overhead---the additional computation needed to decide the probability of whether to update takes only constant time. 

Finally we note that our algorithm can be easily extended to more complex settings. For instance, in Section~\ref{sec:submodapplication} we show how to extended our technique to solve the consistent online submodular maximisation problem, and in Chapter~\ref{chap:rftl} we show similar results hold for regularised online algorithms.

\section{Related Work}
Online Convex Optimisation is a very active research topic in online learning with many beautiful results~\citep{DBLP:journals/tit/MerhavOSW02}. Perhaps the closest to our setting is the work on learning with memory~\citep{DBLP:journals/tit/MerhavOSW02}. 
In this problem, the adversary is oblivious to the algorithm's choices but the loss that the algorithm incurs depends on the current and the recent choices of the algorithm. 
Interestingly, although the setting is different from ours, the algorithms introduced to solve this problem have non-trivial consistency guarantees. Often, the algorithms  are based on a blocking technique~\citep{DBLP:journals/tit/MerhavOSW02} that divides the rounds in blocks and allows only a constant number of switches per block. 
In turn, this technique automatically guarantees also to have a limited consistency cost, but with higher regret\footnote{The regret of this technique is $O(T^{2/3})$ and the consistency cost is of $O(T^{1/3})$.}. 
This result was later improved by~\cite{DBLP:journals/tit/GyorgyN14} using the Shrinking Dartboard framework introduced by~\cite{DBLP:conf/colt/GeulenVW10}, but unfortunately their technique is limited to the expert setting. Recently, \cite{DBLP:conf/nips/AnavaHM15} proposed a new algorithm that obtains near optimal bounds for the Online Convex Optimisation setting in the counterfactual feedback model where the algorithm knows the loss that it would have incurred had it played any sequence of $m$ decisions in the previous rounds. 
Our results are incomparable with these because our algorithm does not assume to have access to counterfactual feedback. Furthermore, it is simpler and can be easily extended to handle more complex settings like the consistent online submodular maximisation problem.

A related area to ours is metrical task systems (MTS). The player's goal is to minimise the movement (in a metric space) plus the costs she recieves, while holding a plausible \emph{competitive ratio}, \ie, the ratio of the cost of the algorithm relative to the cost of the optimal offline algorithm on a worst-case sequence. Important problems in this field include the $k$-server problem \citep{manasse1990competitive,bubeck2017} and convex chasing problem \citep{argue2019nearly}. 

Another area of work that is closely related to ours is research in online algorithms with recourse. In this setting, one seeks better online algorithms to compute optimal or approximate solutions for combinatorial problems by allowing the algorithm to make a limited number of changes to the online solution. 
This concept is very close to the notion of consistency that we consider in this paper. The first problem that received a lot of attention in this area is the classic online Steiner tree problem introduced by~\cite{DBLP:journals/siamdm/ImaseW91} for which it is possible to design better algorithms by allowing a small recourse as shown in several papers \citep{DBLP:journals/siamcomp/GuG016, DBLP:conf/soda/GuptaK14, DBLP:conf/stoc/LackiOPSZ15, DBLP:journals/siamcomp/MegowSVW16}. The concept of recourse has been applied to other classic optimisation problems as online scheduling \citep{DBLP:journals/algorithmica/AndrewsGZ99, DBLP:journals/algorithmica/EpsteinL14, DBLP:journals/algorithmica/PhillipsW98, DBLP:journals/mor/SandersSS09, DBLP:conf/esa/SkutellaV10}, online flow \citep{DBLP:conf/soda/GuptaKS14, DBLP:journals/jal/Westbrook00}, online matching \cite{DBLP:conf/soda/BernsteinHR18} and online set cover \citep{DBLP:conf/stoc/GuptaK0P17}. Very recently, \cite{DBLP:conf/icml/LattanziV17} introduced the notion of consistency in online algorithms for machine learning and studied the consistent online clustering problem.

\section{Contributions}
The main contributions of this thesis can be summarised as follows:
\begin{itemize}
    \item We present the first truly online learning algorithms, capable of dealing with large action sets, that trade-off the frequency with which the solution is updated and the final regret achieved.
    \item We provide explicit regret and consistency trade-offs for Online Gradient Descent, Online Submodular Maximisation, and Regularised Follow The Leader algorithms.
    \item We evaluate our theoretical results on two real-world datasets and show that our proposed method are compatible with, and mostly better than, our theoretical guarantees.
\end{itemize}

Much of this work is presented in \citet{karimi2019}, a joint work with Andreas Krause from ETH Z\"urich, Silvio Lattanzi and Sergei Vassilvitskii from Google, recently published in the 22\textsuperscript{nd} International Conference on Artificial Intelligence and Statistics (AISTATS 2019). Specifically, chapters 1, 3 and 5 are mainly based on the aforementioned paper.

\section{Acknowledgements}
I am grateful for having Prof. Andreas Krause as my supervisor in this research. His constant support and enlightening ideas and excellent guidance were crucial for the success of this work. I also thank Silvio Lattanzi and Sergei Vassilvitskii for our collaboration on this problem and our useful discussions. 

I am also very happy that I have been hosted by the Learning and Adaptive Systems (LAS) group during this research. Specifically, I shall thank Rita Klute for her kind help throughout the process of the thesis.
