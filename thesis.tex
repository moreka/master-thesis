\documentclass[11pt,a4paper]{amsbook}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}

\usepackage{Definitions}

\usepackage[a4paper,twoside,vscale=.7,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{algorithmic}

\definecolor{orangegood}{RGB}{226,147,2}
\definecolor{bluegood}{RGB}{0,92,155}
\hypersetup{
    colorlinks=true,
    linkcolor=bluegood,
    linktoc=page,
    citecolor=bluegood
}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{section}{chapter}
\numberwithin{equation}{chapter}

\newcommand{\from}{\colon}
\newcommand{\ev}{\mathbf{E}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\prob}[1]{\mathop{\mathrm{Pr}}[#1]}

\newcommand{\note}[1]{\textit{\color{cyan}Note: #1}}
\newcommand{\projon}[2]{\operatorname{Proj}_{#1}(#2)}
\newcommand{\proj}[1]{\operatorname{Proj}(#1)}
\setcounter{tocdepth}{1}

\newcommand{\simplex}{\Delta}
\newcommand{\grad}{\nabla}

\usepackage[round]{natbib}
\setcitestyle{authoryear,open={(},close={)}}

\linespread{1.1}

\addto\captionsenglish{\renewcommand{\bibname}{References}}


\begin{document}
\frontmatter

\input{titlepage}
\setcounter{page}{0}
\clearpage\thispagestyle{empty}\mbox{}\clearpage
\title{Consistent Online Optimisation}
%    Information for first author
\author{Mohammad Reza Karimi Jaghargh}
%    Address of record for the research reported here
\address{ETH Z\"urich, Switzerland}
%    Current address
\email{mkarimi@ethz.ch}
\maketitle

\begin{abstract}
    Modern online learning algorithms achieve low (sublinear) regret in a variety of diverse settings. These algorithms, however, update their solution at every time step. While these updates are computationally efficient, the very requirement of frequent updates makes the algorithms untenable in practical applications where the change of solution is costly. In this work, we develop online learning algorithms that update their solution a sublinear number of times. We give a meta-algorithm based on non-homogeneous Poisson processes that gives a smooth trade-off between the regret and frequency of updates. Empirically, we show that in our two real-world examples, one can significantly reduce updates at a minimal increase in the regret.
\end{abstract}

\setcounter{page}{4}
\tableofcontents

\mainmatter

\input{ch1-introduction}
\input{ch2-prelim}
\input{ch3-consistent}
\input{ch35-regularisation}
\input{ch4-experiments}
\input{ch5-conclusion}

\appendix
\input{app-proofs-ch2}

\backmatter

\bibliographystyle{agsm}    % or amsalpha
\bibliography{thesis}

\newpage
\includepdf[pages={-}]{declaration-originality.pdf}
\end{document}
