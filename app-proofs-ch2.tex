\chapter{Supplementary Material}
\begin{lemma}[Projection Lemma]\label{lem:projection}
    Let $X$ be a nonempty convex closed set in $\RR^d$, $x\in \RR^d$ be a point and $z$ be any point in $X$. One has
    \[
        \| \proj{x} - z \| \leq \|x - z\|.
    \]
\end{lemma}
\begin{proof}
    Let $y = \proj{x}$. If $y=x$, then the claim is clear. So we assume that $x\not\in X$. We first note that $\inner{y-x}{y-z} \leq 0$. This is because $y$ is the projection of $x$ onto a convex set, and the hyperplane perpendicular to $x-y$ and passing through $y$ separates $z$ from~$x$.

    For $\alpha \in [0,1]$ define $z_\alpha := z + \alpha(y-z)$. By convexity, $z_\alpha \in X$. Define
    \[
        f(\alpha) := \|z_\alpha - x\|^2 - \|z_\alpha - y\|^2. 
    \]
    We wish to prove that $f(0) \leq 0$.  As $z_1 = y$, we have $f(1) = 0$. Thus, it suffices to prove that $f'(\alpha) \leq 0$ for all $\alpha\in[0,1]$. We have
    \[
        f(\alpha) = \|z-x\|^2 - 2\alpha\inner{y-z}{x-z} - (1-2\alpha)\|z-y\|^2,
    \]
    and
    \[
        f'(\alpha) = 2\inner{y-z}{y-x} \leq 0.
    \]
    Hence, we are done.
\end{proof}

\section{Another Proof for Consistent Experts}
Let $\simplex$ denote the $n$-simplex. Assume the sequence of functions $(f_t)_{t\in[T]}$ with $f_t\from\simplex\to \RR$ are Lipschitz continuous with respect to 1-norm, that is, for all $x\in\simplex$, we have $\|\grad f(x)\|_\infty \leq G_\infty$, for some $G_\infty>0$.

The Exponentiated Gradient algorithm, gives sublinear regret, as stated in the following theorem:
\begin{theorem}
    Choose $x_1$ to be the center of the simplex. Then, at round $t$ play $x_t$ and update
    \[
        x_{t+1, i} = \frac{1}{Z}\cdot x_{t,i} \cdot e^{-\eta (\grad f_t(x_t))_i},
    \]
    where $Z$ is a normalising constant such that $x_{t+1}\in \simplex$. Playing such, with $\eta = \frac{1}{G_\infty} \sqrt{\frac{\log d}{T}}$ gives
    \[
        \mathrm{regret}_T \leq 2G_\infty \sqrt{T \log d}.
    \]
\end{theorem}

We now note that the points $x_t$ does not move that fast in $\simplex$. By Pinsker's inequality, we have
\[
    2\|x_{t+1} - x_t\|_1^2 \leq \mathrm{KL}(x_{t+1}\|x_t).
\]
Thus, it suffices to bound the KL divergence between $x_{t+1}$ and $x_t$. For simplicity, set $\grad_t := \grad f_t(x_t)$. We have
\begin{align*}
    \mathrm{KL}(x_{t+1}\|x_t) &= \sum_{i=1}^d x_t^i \log\frac{x_t^i}{x_{t+1}^i} \\
                              &= \sum_{i=1}^d x_t^i \cdot (\log x_t^i + \log Z - \log x_{t}^i + \eta \grad_t^i) \\
                              &= \log Z + \eta \inner{x_t}{\grad_t}.
\end{align*}
\begin{lemma}
    If $\eta \leq 1/G_\infty$, it holds that
    \[
        \log Z \leq -\eta\inner{x_t}{\grad_t} + \eta^2 G_\infty^2.
    \]
\end{lemma}
\begin{proof}
    Using the fact that for $x \leq 1$ it holds $e^x \leq 1 + x + x^2$, we get
    \begin{align*}
        \log Z &= \log \sum_{i=1}^d x_t^i \cdot e^{-\eta \grad_t^i} \\
               &\leq \log \sum_{i=1}^d x_t^i\cdot (1 - \eta\grad_t^i + \eta^2(\grad_t^i)^2) \\
               &\leq \log (1 - \eta\inner{x_t}{\grad_t} + \eta^2G_\infty^2) \\
               &\leq - \eta\inner{x_t}{\grad_t} + \eta^2G_\infty^2 \qedhere
    \end{align*}
\end{proof}

Now by the lemma we get our desired result. In summary, we have the following.
\begin{proposition}
    Let $(x_t)_{t\leq T}$ denote the sequence played by the EG algorithm. It holds
    \[
        \|x_{t+1} - x_t\|_1 \leq \frac{1}{\sqrt{2}}\,\eta\, G_\infty.
    \]
\end{proposition}

%The rest of the argument is the same as the one described in Section~\ref{sec:experts}.
